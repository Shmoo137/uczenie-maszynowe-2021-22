{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Kopia notatnika 09_Uczenie_bez_nadzoru.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Algorytm k-means #\n",
        "\n",
        "Algorytm k-means jest zaimplementowany w module *scipy.cluster.vq* ([*vq: vector quantization*](https://docs.scipy.org/doc/scipy/reference/cluster.vq.html)). Mamy tam funkcję:\n",
        "\n",
        "*kmeans(obs, k_or_guess, iter=20, thresh=1e-05)*\n",
        "\n",
        "optymalizującą położenia centroidów, oraz pomocniczą funkcję *vq*, przypisującą poszczególne obserwacje do skupisk reprezentowanych przez centroidy.\n",
        "\n",
        "Przed zastosowaniem algorytmu k-means na danych dobrze jest przeskalować każdą z cech w macierzy wejściowej, tak aby miała jednostkową wariancję. Można to zrobić za pomoca funkcji [*whiten*](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.vq.whiten.html). Ten rodzaj normalizacji jest dedykowany do k-means.\n",
        "\n",
        "###Przykładowy kod.###\n",
        "Kod ten pokazuje jak:\n",
        "\n",
        "*   wygenerować symulowane dane,\n",
        "*   przeskalować je, tak aby miały jednostkową wariancję w każdej z cech,\n",
        "*   podzielić je na dwa skupiska,\n",
        "*   zilustrować wynik."
      ],
      "metadata": {
        "id": "Zdu6eCwycpM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from numpy import vstack,array\r\n",
        "from numpy.random import rand\r\n",
        "from scipy.cluster.vq import kmeans, vq, whiten\r\n",
        " \r\n",
        "# generujemy dane: \r\n",
        "# 150 dwuwymiarowych punktów z rozkładu jednorodnego ze średnią (1,1)\r\n",
        "# 150 dwuwymiarowych punktów z rozkładu jednorodnego ze średnią  (0.5,0.5)\r\n",
        " \r\n",
        "data = vstack((rand(150,2) + array([.5,.5]),rand(150,2)))\r\n",
        "data = whiten(data) # obowiązkowy krok dla implementacji kmeans w scipy! Patrz: dokumentacja!\r\n",
        "# policz K-Means dla K = 2 (2 skupiska)\r\n",
        "centroids, _ = kmeans(data, 2)\r\n",
        "# przypisz wektory wejściowe do skupisk\r\n",
        "idx, _ = vq(data, centroids)\r\n",
        " \r\n",
        "# narysuj wyniki\r\n",
        "plt.plot(data[idx==0,0], data[idx==0,1], 'ob', data[idx==1,0], data[idx==1,1], 'or')\r\n",
        "plt.plot(centroids[:,0], centroids[:,1], 'sg', markersize=8)\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "vbzIoPw7c4an",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1fbe20bb-eb6a-41ef-c448-e90d9223c35e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Segmentacja obrazu algorytmem k-means#\n",
        "\n",
        "W tym ćwiczeniu zapoznamy się z zastosowaniem algorytmu analizy skupień do segmetacji obrazu. Segmentacja tego typu może stanowić etap wstępnego przetwarzania na potrzeby np. detekcji obiektów lub klasyfikacji. W zadaniu tym zapoznamy sie także z metodą dobierania atumatycznie ilości skupisk.\n",
        "\n",
        "\n",
        "Obrazek na którym będziemy pracować znajduje się pod [tym](https://brain.fuw.edu.pl/edu/images/b/b8/Skan.png) adresem, ale możecie go też pobrać z GitHuba:"
      ],
      "metadata": {
        "id": "bWC1XGKNdix4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!git clone https://github.com/Shmoo137/uczenie-maszynowe-2021-22"
      ],
      "outputs": [],
      "metadata": {
        "id": "jUPM2Q7Zdd2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597d1619-8393-498c-9afb-d963a585206a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "folder = '/content/uczenie-maszynowe-2021-22/dane/' # tu się domyślnie zapisuje pobrane repo z GitHuba, a dane są w folderze \"dane\""
      ],
      "outputs": [],
      "metadata": {
        "id": "33zlikdAeLDs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from scipy.cluster.vq import kmeans,vq\r\n",
        " \r\n",
        "image = plt.imread(folder + 'image.png')\r\n",
        "plt.axis('off')\r\n",
        "plt.imshow(image)"
      ],
      "outputs": [],
      "metadata": {
        "id": "othT-oHbeWtE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "ea1b94bd-5bf6-4959-bc10-4690d143ab79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obrazek jest w skali szarości, co oznacza, że każdy piksel zakodowany w RGB ma te same wartości w każdym kanale. Możecie to dodatkowo sprawdzić. W związku z tym nie potrzebujemy wszystkich trzech kanałów. Od teraz pracuj na macierzy danych pochodzących tylko z jednego kanału."
      ],
      "metadata": {
        "id": "foXnDEuWgzzY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data = "
      ],
      "outputs": [],
      "metadata": {
        "id": "7JHXlLmYg0ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teraz zamieniamy rysunek (dwuwymiarowa tablica 256x256) na wektor (o długości 256*256):"
      ],
      "metadata": {
        "id": "GzBdcj8Wej95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data = "
      ],
      "outputs": [],
      "metadata": {
        "id": "2jFSfqhleama"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jak widzieliśmy powyżej, za pomocą kmeans możemy podzielić te dane na k skupisk. Ale jaką liczbę skupisk wybrać? Jest wiele metryk, które nam w tym mogą pomóc, ale my zastosujemy prosty indeks Dunn/Dunna (nie mogę znaleźć płci autora indeksu :P). Najlepsza liczba skupisk dla danego problemu ma najwyższy D-indeks.\n",
        "\n",
        "$D(k) = \\frac{J_{\\mathrm{inter}}(k)}{J_{\\mathrm{intra}}(k)}$\n",
        "\n",
        "Dla każdej liczby skupisk $k$ obliczamy więc dwie wielkości:\n",
        "\n",
        "$J_{intra} (k)$ - to jest miara odległości wewwnątrz centrów: równanie na $J$ znajduje się [tutaj](https://brain.fuw.edu.pl/edu/index.php/Uczenie_maszynowe_i_sztuczne_sieci_neuronowe/Wyk%C5%82ad_10#Algorytm_k-.C5.9Brednich:). Dostajemy je jako output kmeans.\n",
        "\n",
        "$J_{inter} (k) = \\mathrm{min}_{j<i} \\sqrt{(\\mu_i - \\mu_j)^T (\\mu_i - \\mu_j)} $ - to najmniejsza odległość między dwoma centrami (spośród $k$). Musimy znaleźć ją sami. Podpowiedź: równanie jest podane dla wielowymiarowych danych, ale nasze są jednowymiarowe, więc możemy je sobie uprościć!"
      ],
      "metadata": {
        "id": "tNUF69V_exM8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Innymi słowy chcemy przeprowadzić kmeans dla K = 2, 3, ..., K_max liczby skupisk.\r\n",
        "# kmeans zwraca nam środki K centroid oraz J_intra, czyli odległości wewnątrz centrów\r\n",
        "# Sami będziemy musieli policzyć J_inter dla każdego kroku pętli.\r\n",
        "# Do kolejnych obliczeń (próby znalezienia optymalnej liczby skupisk), potrzebujemy zapisać dla każdego K:\r\n",
        "# 1) J_inter dla każdego K oraz 2) J_intra dla każdego K\r\n",
        "\r\n",
        "# Zacznij od kroku obowiązkowego dla kmeans, czyli przeskaluj dane tak, by miały jednostkową wariancję cech (whiten)\r\n",
        "\r\n",
        "# Zadeklaruj maksymalną rozpatrywaną liczbę skupisk\r\n",
        "K_max = \r\n",
        "J_inter = # do zapisywania wyników\r\n",
        "J_intra = # do zapisywania wyników \r\n",
        "centroids = []\r\n",
        "\r\n",
        "# Pętla po różnych k\r\n",
        "\r\n",
        "  # puść kmeans, zapisz J_intra\r\n",
        "\r\n",
        "  # policz J_inter, czyli policz odległości między centrami wszystkich par skupisk i zapisz najmniejszą"
      ],
      "outputs": [],
      "metadata": {
        "id": "W1ML6sZlmgwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wykreślamy D-indeks i znajdujemy $k$, dla którego jest największy:"
      ],
      "metadata": {
        "id": "TWRVl9gDgTAK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure()\r\n",
        "plt.plot(range(2, K_max), J_inter[]/J_intra[])\r\n",
        "K_opt = \r\n",
        "print(K_opt)"
      ],
      "outputs": [],
      "metadata": {
        "id": "OlrrmnUafHsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dla tej optymalnej ilości skupisk znajdujemy położenia centrów i przypisujemy klasę dla każdego punktu danych:"
      ],
      "metadata": {
        "id": "_ijpVFvBgefI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "centroids, J_intra[k] = kmeans(data, K_opt)\r\n",
        " \r\n",
        "# przypisujemy klasę\r\n",
        "idx,_ = vq(data,centroids)"
      ],
      "outputs": [],
      "metadata": {
        "id": "qm8wsPg3gXyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formatujemy wektor w obrazek i podziwiamy efekt:"
      ],
      "metadata": {
        "id": "JRDHLYH-gh_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "idx.shape = 256,256\r\n",
        "plt.figure()\r\n",
        "plt.imshow(idx, cmap=cm.gray)\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "wLwa3jSigjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dla porównania proszę wykreślić histogram odcieni szarości dla wektora *data* i wektora *idx*."
      ],
      "metadata": {
        "id": "TtE7o56zgohN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "E4Xnq3hNghq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na zakończenie, napiszcie proszę własnymi słowami jak działa algorytm k-means. Może być kolokwialnie ;)"
      ],
      "metadata": {
        "id": "7WVAAMC0gqlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "GHa4bzjpgq1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pamiętajcie, że indeks D nie wyczerpuje sposobów na znajdywanie optymalnej liczby klastrów :)"
      ],
      "metadata": {
        "id": "iP0j54K6uDSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Algorytm EM (Expectation Maximization) - dla chętnych#\n",
        "\n",
        "Program, który powstanie po uzupełnieniu kodu powinien ilustrować dopasowywanie modelu EM do danych będzących sumą dwóch rozkładów gaussowskich.\n",
        "\n",
        "Najpierw standardowe importy i kilka funkcji pomocniczych:"
      ],
      "metadata": {
        "id": "WbZ-2Ajygvjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib\r\n",
        "import pylab as py\r\n",
        "import random, copy\r\n",
        "import numpy as np\r\n",
        "import sys\r\n",
        " \r\n",
        " \r\n",
        "def pnorm(x, m, s):\r\n",
        "    \"\"\"\r\n",
        "    Oblicza gęstość wielowymiarowego rozkładu normalnego dla punktów\r\n",
        "    w wektorze x\r\n",
        "    Parametry rozkładu:\r\n",
        "    m - średnia\r\n",
        "    s- macierz kowariancji\r\n",
        "    dla zwiększenia czytelności kodu stosujemy typ matrix\r\n",
        "    \"\"\"\r\n",
        "    xm = np.matrix(x-m)\r\n",
        "    xmt = np.matrix(x-m).transpose()\r\n",
        "    for i in range(len(s)):\r\n",
        "        if s[i,i] <= sys.float_info[3]: # min float\r\n",
        "            s[i,i] = sys.float_info[3]\r\n",
        "    sinv = np.linalg.inv(s)\r\n",
        " \r\n",
        "    return (2.0*np.pi)**(-len(x)/2.0)*(1.0/np.sqrt(np.linalg.det(s)))\\\r\n",
        "            *np.exp(-0.5*(xm*sinv*xmt))\r\n",
        " \r\n",
        "def draw_params(t,nbclusters):\r\n",
        "        '''funkcja do losowania parametrów początkowych\r\n",
        "        t - zbiór treningowy\r\n",
        "        '''\r\n",
        "        nbobs,nbfeatures = t.shape\r\n",
        "        # inicjuje średnie przez losowanie punktu ze zbioru danych\r\n",
        "        tmpmu = np.array([t[np.random.randint(0,nbobs),:]],np.float64)\r\n",
        "        # kowariancje inicjowane są jako macierze diagonalne , wariancja dla każdej cechy inicjowana jest jako wariancja tej cechy dla całego zbioru \r\n",
        "        sigma = np.zeros((nbfeatures,nbfeatures))\r\n",
        "        for f in range(nbfeatures):\r\n",
        "            sigma[f,f] = np.var(t[:,f])\r\n",
        "        #phi inicjujemy tak, że każda składowa mieszanki ma takie samee prawdopodobieństwo\r\n",
        "        phi = 1.0/nbclusters\r\n",
        "        print ('INIT:', tmpmu, sigma, phi)\r\n",
        "        return {'mu': tmpmu,\\\r\n",
        "                'sigma': sigma,\\\r\n",
        "                'phi': phi}\r\n",
        " \r\n",
        "def plot_gauss(mu,sigma):\r\n",
        "    ''' Funkcja rysująca kontury funkcji gęstości prawdopodobieństwa \r\n",
        "       dwuwymiarowego rozkładu Gaussa'''\r\n",
        " \r\n",
        "    x = np.arange(-6.0, 6.0001, 0.1)\r\n",
        "    y = np.arange(-6.0, 6.0001, 0.1)\r\n",
        "    X,Y = np.meshgrid(x, y)\r\n",
        "    X.shape = 1,len(x)*len(y)\r\n",
        "    Y.shape = 1,len(x)*len(y)\r\n",
        "    P = np.vstack((X,Y))\r\n",
        "    invS = np.linalg.inv(sigma)\r\n",
        "    R = P.T-mu\r\n",
        "    z = np.zeros(len(R))\r\n",
        "    for i in range(len(R)):\r\n",
        "        z[i] = np.exp(-0.5*np.dot( R[i,:].T,np.dot(invS,R[i,:])))\r\n",
        " \r\n",
        "    z.shape = len(x),len(y)\r\n",
        "    py.contourf(x,y,z,alpha = 0.5)\r\n",
        "    py.plot(mu[0],mu[1],'o')"
      ],
      "outputs": [],
      "metadata": {
        "id": "6PAGNYwogn5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Szkielet algorytmu ##\n",
        "Poniższy kod to szkielet właściwej funkcji wykonującej optymalizację. Trzeba go uzupełnić implementując równania z wykładu. Proszę uważnie czytać komentarze."
      ],
      "metadata": {
        "id": "LJjStgY9hogw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def expectation_maximization(t, nbclusters=2, nbiter=3, normalize=False,\\\n",
        "        epsilon=0.001, monotony=False, datasetinit=True):\n",
        "    \"\"\"\n",
        "    t - zbiór treningowy, \n",
        "    Każdy wiersz t jest przykładem (obserwacją), każda kolumna to cecha \n",
        "    'nbclusters' ilość klastrów, z których budujemy model mieszany\n",
        "    'nbiter' ilość iteracji\n",
        "    'epsilon' kryterium zbieżności\n",
        " \n",
        "     Powtórz kroki E i M aż do spełnienia warunku |E_t - E_{t-1}| < ε\n",
        " \n",
        "    Funkcja zwraca parametry modelu (centra i macerze kowariancji Gaussów i ich wagi \\phi) oraz \n",
        "    etykiety punktów zbioru treningowego oznaczające do którego z Gaussów w modelowanej mieszance należą.\n",
        "    \"\"\"\n",
        " \n",
        "    nbobs,nbfeatures = t.shape\n",
        " \n",
        "    ### Opcjonalna normalizacja\n",
        "    if normalize:\n",
        "        for f in range(nbfeatures):\n",
        "            t[:,f] /= np.std(t[:,f])\n",
        " \n",
        " \n",
        "    result = {}\n",
        "    random.seed()\n",
        " \n",
        "    # szykujemy tablice na prawdopodobieństwa warunkowe\n",
        "    Pz = np.zeros((nbobs,nbclusters)) # P(z|x): opisywane równaniami (2) i (3) z wykładu \n",
        "    Px = np.zeros((nbobs,nbclusters)) # P(x|z): opisywane równaniem (4)  \n",
        " \n",
        "    # inicjujemy parametry dla każdego składnika mieszanki\n",
        "    # params będzie listą taką, że params[i] to słownik\n",
        "    # zawierający parametry i-tego składnika mieszanki\n",
        "    params = []\n",
        "    for i in range(nbclusters):\n",
        "        params.append( draw_params(t,nbclusters) )\n",
        " \n",
        "    old_log_estimate = sys.maxsize                   # init\n",
        "    log_estimate = sys.maxsize/2 + epsilon      # init\n",
        "    estimation_round = 0    \n",
        " \n",
        "    # powtarzaj aż zbiegniesz \n",
        "    while (abs(log_estimate - old_log_estimate) > epsilon\\\n",
        "                and (not monotony or log_estimate < old_log_estimate)):\n",
        "        restart = False\n",
        "        old_log_estimate = log_estimate   \n",
        "        ########################################################\n",
        "        # krok E: oblicz Pz dla każdego przykładu (czyli w oznaczeniach z wykładu w_i^j)\n",
        "        ########################################################\n",
        "        # obliczamy prawdopodobieństwa  Px[j,i] = P(x_j|z_j=i)  \n",
        "        for j in range(nbobs): # iterujemy po przykładach\n",
        "            for i in range(nbclusters): # iterujemy po składnikach\n",
        "                Px[j,i] = pnorm(t[j,:], params[i]['mu'], params[i]['sigma']) #  (równanie 4)\n",
        " \n",
        "        #  obliczamy prawdopodobieństwa Pz[j,i] = P(z_j=i|x_j)   \n",
        "        #  najpierw licznik równania (3)   \n",
        "        for j in range(nbobs): \n",
        "            for i in range(nbclusters):\n",
        "                Pz[j,i] = .............*params[i]['phi']\n",
        "        #  mianownik równania (3)\n",
        "        for j in range(nbobs): \n",
        "            tmpSum = 0.0\n",
        "            for i in range(nbclusters):\n",
        "                tmpSum += ..................\n",
        "        # składamy w całość Pz[j,i] = P(z_j=i|x_j)\n",
        "            Pz[j,:] /= tmpSum\n",
        " \n",
        "        ###########################################################\n",
        "        # krok M: uaktualnij paramertry (sets {mu, sigma, phi}) #\n",
        "        ###########################################################\n",
        "        #print \"iter:\", iteration, \" estimation#:\", estimation_round,\\\n",
        "        #            \" params:\", params\n",
        "        for i in range(nbclusters):\n",
        "            print(\"------------------\")\n",
        "            # parametr phi: równanie (6)\n",
        "            Sum_w = np.sum(Pz[:,i])\n",
        "            params[i]['phi'] = Sum_w/nbobs\n",
        "            if params[i]['phi'] <= 1.0/nbobs:           # restartujemy jeśli zanika nam któraś składowa mieszanki\n",
        "                restart = True                          \n",
        "                print(\"Restarting, p:\",params[i]['phi'])\n",
        "                break\n",
        "            print('i: ',i,' phi: ', params[i]['phi'])\n",
        "            # średnia: równanie (7)\n",
        "            m = np.zeros(nbfeatures)\n",
        "            for j in range(nbobs):\n",
        "                m += ......................\n",
        "            params[i]['mu'] = m/Sum_w\n",
        "            print('i: ',i,' mu: ', params[i]['mu'])\n",
        " \n",
        "            # macierz kowariancji: równanie (8)\n",
        "            s = np.matrix(np.zeros((nbfeatures,nbfeatures)))\n",
        "            for j in range(nbobs):\n",
        "                roznica = np.matrix(t[j,:]-params[i]['mu'])\n",
        "                s += Pz[j,i]*(roznica.T*roznica)\n",
        "            params[i]['sigma'] = s/Sum_w\n",
        " \n",
        "            print(params[i]['sigma'])\n",
        " \n",
        "            ### Testujemy czy składniki się nie sklejają i w razie potrzeby restartujemy\n",
        "            if not restart:\n",
        "                restart = True\n",
        "                for i in range(1,nbclusters):\n",
        "                    if not np.allclose(params[i]['mu'], params[i-1]['mu'])\\\n",
        "                    or not np.allclose(params[i]['sigma'], params[i-1]['sigma']):\n",
        "                        restart = False\n",
        "                        break\n",
        "            if restart:                \n",
        "                old_log_estimate = sys.maxsize                 # init\n",
        "                log_estimate = sys.maxsize/2 + epsilon    # init\n",
        "                params = [draw_params(t,nbclusters) for i in range(nbclusters)] # losujemy nowe parametry startowe\n",
        "                print('RESTART')\n",
        "                continue\n",
        " \n",
        " \n",
        "            ####################################\n",
        "            # liczymy estymatę log wiarygodności: równaie (1)  #\n",
        "            ####################################\n",
        "            log_estimate = np.sum([np.log(np.sum(\\\n",
        "                    [Px[j,i]*params[i]['phi'] for i in range(nbclusters)]))\\\n",
        "                    for j in range(nbobs)])\n",
        "            print(\"(EM) poprzednia i aktualna estymata log wiarygodności: \",\\\n",
        "                    old_log_estimate, log_estimate)\n",
        "            estimation_round += 1\n",
        "        ##########################\n",
        "        #  rysujemy aktualny stan modelu\n",
        "        ##########################\n",
        "        py.ioff()\n",
        "        py.clf()\n",
        "        py.ion()\n",
        "        for i in range(nbclusters):\n",
        "            plot_gauss(np.array(params[i]['mu']),np.array(params[i]['sigma']))\n",
        "        py.plot(x[:,0],x[:,1],'g.')\n",
        "        py.axis('equal')\n",
        "        py.draw()\n",
        " \n",
        " \n",
        "        # Pakujemy wyniki\n",
        "            result['quality'] = -log_estimate\n",
        "            result['params'] = copy.deepcopy(params)\n",
        "            result['clusters'] = [[o for o in range(nbobs)\\\n",
        "                    if Px[o,c] == max(Px[o,:])]\\\n",
        "                    for c in range(nbclusters)]\n",
        "    return result"
      ],
      "outputs": [],
      "metadata": {
        "id": "kfqsAxKHg-PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finalny program ##\n",
        "Przykładowy program korzystający z powyższych funkcji:"
      ],
      "metadata": {
        "id": "GNj17_QjiKOW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# robimy mieszankę dwóch gaussów:\n",
        "#parametry rozkładu\n",
        "# wektor średnich:\n",
        "mu1 = [-2,-3] \n",
        "# macierz kowariancji:\n",
        "Sigma1 = np.array([[1, 0.5],\n",
        "                  [0.5, 1]])\n",
        "# generujemy dane: \n",
        "x1 = np.random.multivariate_normal(mu1, Sigma1, 150) #\n",
        "mu2 = [-0.5,2] \n",
        "# macierz kowariancji:\n",
        "Sigma2 = np.array([[3, 0.5],\n",
        "                  [0.5, 1]])\n",
        "# generujemy dane: \n",
        "x2 = np.random.multivariate_normal(mu2, Sigma2, 150) #\n",
        "# łączymy x1 i x2 aby otrzymac jeden zbiór\n",
        "x = np.vstack((x1,x2))\n",
        "py.plot(x[:,0],x[:,1],'g.')\n",
        "py.axis('equal')\n",
        "py.show()\n",
        "py.figure()\n",
        "res = expectation_maximization(x, nbclusters=2, nbiter=3, normalize=False,\\\n",
        "        epsilon=0.001, monotony=False, datasetinit=True)\n",
        "py.ioff()\n",
        "py.show()\n",
        "# wypisz parametry\n",
        "print('Dopasowany model: ')\n",
        "print(res['params'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "BxEgccceiOcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aby obliczyć gęstość prawdopodobieństwa rozkładu mieszanego dla pewnego nowego punktu \"x\" możemy zastosować poniższą funkcję:"
      ],
      "metadata": {
        "id": "F6bCHBu9iUjw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def prob_mix(params, x):\n",
        "    '''params - parametry dopasowanego gaussowskiego modelu mieszanego\n",
        "    x - punkt wejścowy,\n",
        " \n",
        "    funkcja zwraca gestość prawdopodobieństwa, dla x w rozkładzie mieszanym\n",
        "    '''\n",
        "    prob = 0\n",
        "    for i in range(len(params)):\n",
        "        prob+= pnorm(x, params[i]['mu'], params[i]['sigma']) * params[i]['phi']\n",
        " \n",
        " \n",
        "    return prob\n",
        "#---------------- przykładowe użycie: ----------------\n",
        "x=(6,-4)\n",
        "print('P(x=(',str(x),')):', prob_mix(res['params'], x))"
      ],
      "outputs": [],
      "metadata": {
        "id": "vviQVJYmiWXU"
      }
    }
  ]
}