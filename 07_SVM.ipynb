{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07_SVM.ipynb","provenance":[{"file_id":"1EgEr6hLBxUvEfBgXI847RLs4TXBH-oLk","timestamp":1607080033588},{"file_id":"13Q6pB7R8v-BkdMwCyMrJUJui5mU-k9wi","timestamp":1574347142205}],"collapsed_sections":[]},"kernelspec":{"display_name":"ENV","language":"python","name":"env"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XRRmZQCjce0n"},"source":["# Klasyfikacja za pomocą algorytmu wektorów wspierających (SVM)\n","## Materiały\n","Na tych ćwiczeniach zapoznamy się z zastosowaniem SVM do klasyfikacji.\n","Notebook bazuje na tutorialu w [Medium](https://medium.com/swlh/visualizing-svm-with-python-4b4b238a7a92)."]},{"cell_type":"code","metadata":{"id":"Z9elf7UFce0p"},"source":["import numpy as np\n","import matplotlib.pylab as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i0Y7co2kce0t"},"source":["#### Importujemy dane uczące, znów iryski, ale tym razem skorzystamy do importu z biblioteki Seaborn.\n","Daje nam to dodatkowe wygodne narzędzia jak \"drop\", \"pairplot\", ale też zmienia format danych, jako że korzysta z frameworku 'pandas'. Nie utrudnia to znacząco życia, ale czasem wymaga dodatkowej uwagi. Będę ostrzegać :)"]},{"cell_type":"code","metadata":{"id":"ftGa5W32ce0u"},"source":["import seaborn as sns\n","iris = sns.load_dataset(\"iris\")\n","print(iris.head())\n","y = iris.species\n","X = iris.drop('species',axis=1)\n","sns.pairplot(iris, hue=\"species\",palette=\"bright\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vceLTaCZHH9t"},"source":["## Liniowo separowalne dane"]},{"cell_type":"markdown","metadata":{"id":"FOhagrM0ce0w"},"source":["#### Ograniczamy się dziś do dwóch cech i dwóch klas irysów. \n","\n","W pierwszej części ćwiczeń chciałabym wizualizować nasze wyniki, by złapać podstawową intuicję za działaniem SVM. Wyłącznie w celu ułatwienia wizualizacji ograniczymy się więc na razie do dwóch cech i dwóch klas. Np. weźmy cechy petal_width, petal_length oraz klasy Setosa i Versicolor.\n","    "]},{"cell_type":"code","metadata":{"id":"QFEsI3JGce0w"},"source":["data = iris[(iris['species']!='virginica')]\n","data = data.drop(['sepal_length','sepal_width'], axis=1)\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-3albqE7ce0z"},"source":["Zmienimy teraz nazwy kategorii na liczby i zwizualizujemy, na czym będziemy dzisiaj pracować :)"]},{"cell_type":"code","metadata":{"id":"d1pTWJToce0z"},"source":["data = data.replace('setosa', 0)\n","data = data.replace('versicolor', 1)\n","X = data.iloc[:,0:2] # the 'iloc' to właśnie konsekwencja korzystania z pandas. Gdyby data było zwykłym arrayem, starczyłoby data[:,0:2] :)\n","y = data['species']\n","plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, s=50, cmap='autumn')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAXJD12Zce02"},"source":["Czas na trening modelu. Jak może już zauważyliście, ten etap w sklearn jest bardzo podobny niezależnie od modelu. Analogicznie wyglądał dla naiwnego Bayesa!"]},{"cell_type":"code","metadata":{"id":"rxtvTDUYy1Bj"},"source":["from sklearn.svm import SVC \n","model = SVC(kernel='linear', C=1E10)\n","model.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BPF3lsfiDptR"},"source":["Po raz pierwszy na ćwiczeniach dotykamy tematu regularyzacji modelu. Regularyzacja to ogólnie rzecz biorąc każda technika, która zwiększa zdolność generalizacji modelu. Będziemy o tym mówić szczególnie w kontekście sieci neuronowych. Tutaj takim parametrem regularyzacyjnym jest parametr C, regulujący koszt \"naruszania\" marginesów zbudowanych przez model. Im mniejszy parametr C, tym mniejszy koszt i większa szansa, że jakiś"]},{"cell_type":"markdown","metadata":{"id":"3hdntvibEr2r"},"source":["#### Możemy zwizualizować wekstory wspierające, na których zbudowany jest nasz model"]},{"cell_type":"code","metadata":{"id":"uAtnmWzCce04"},"source":["print(model.support_vectors_)\n","\n","plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, s=50, cmap='autumn')\n","plt.scatter(model.support_vectors_[:,0],model.support_vectors_[:,1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ayuMCXo4ce06"},"source":["#### Narysujmy te dane"]},{"cell_type":"code","metadata":{"id":"8eFWx1xSce08"},"source":["ax = plt.gca()\n","\n","# Rysujemy wszystkie dane treningowe\n","plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, s=50, cmap='autumn')\n","\n","# Budujemy siatkę (meshgrid)\n","xlim = ax.get_xlim()\n","ylim = ax.get_ylim()\n","xx = np.linspace(xlim[0], xlim[1], 30)\n","yy = np.linspace(ylim[0], ylim[1], 30)\n","YY, XX = np.meshgrid(yy, xx)\n","xy = np.vstack([XX.ravel(), YY.ravel()]).T\n","\n","# Rysujemy funkcję decyzyjną nauczonego modelu\n","Z = model.decision_function(xy).reshape(XX.shape)\n","ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n","           linestyles=['--', '-', '--'])\n","\n","# Dorysowywujemy wektory wspierające\n","support_vector1 = model.support_vectors_[:, 0]\n","support_vector2 = model.support_vectors_[:, 1]\n","ax.scatter(support_vector1, support_vector2, s=100,\n","           linewidth=1, facecolors='none', edgecolors='k')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TNL4OFikce0_"},"source":["#### Co się stanie z wyznaczoną funkcją decyzji jak usuniemy część danych uczących?\n","\n","Korzystając z kawałka już napisanego kodu, który buduje Wam dane uczące biorąc wszystkie dane z żółtej klasy versicolor i losuje 70% danych z czerwonej klasy setosa, przetrenujcie kilka razy SVM, rysując funkcję decyzji (jak wyżej). Jakie macie wnioski?\n","\n","PS. Najlepiej zróbcie sobie z tego funkcję z argumentami: X, y, kernel, C, gamma, bo będziecie tego używać jeszcze kilka razy :)"]},{"cell_type":"code","metadata":{"id":"rMDws_lmce1A"},"source":["def trainSVM_and_visualize(X_train, Y_train, kernel, C=1.0, gamma='scale'):\n","  # Trening modelu\n","  ...\n","\n","  # Deklarujemy obiekt wykres\n","  ...\n","\n","  # Rysujemy wszystkie dane treningowe\n","  ...\n","\n","  # Budujemy siatkę (meshgrid)\n","  ...\n","\n","  # Rysujemy funkcję decyzyjną nauczonego modelu\n","  ...\n","\n","  # Dorysowywujemy wektory wspierające\n","  ...\n","\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m-C3IQ3xSAhD"},"source":["# Przygotowanie danych uczących\n","red_sample = data.sample(frac=0.7)\n","X = red_sample.iloc[:,0:2]\n","y = red_sample['species']\n","\n","trainSVM_and_visualize(X, y, 'linear')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AWXe8Ec8ce1E"},"source":["## Liniowo nieseparowalne dane"]},{"cell_type":"markdown","metadata":{"id":"U5SbxC4CHWTQ"},"source":["Aby zwizualizować super moc SVM na danych nieseparowalnych liniowo, to zaczniemy od sztucznie wygenerowanych danych."]},{"cell_type":"code","metadata":{"id":"53D6GV8fce1F"},"source":["from sklearn.datasets.samples_generator import make_circles\n","X, y = make_circles(100, factor=.1, noise=.1)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJPA8b6_ce1H"},"source":["#### Wytrenujcie na takich danych SVM z liniowym kernelem i zwizualizujcie wyniki\n","Proponuję użyć do tego tej samej funkcji co stworzyliście ostatnio, ALE dodajcie dodatkowy argument opcjonalny np. 'panda = True' od którego zależeć będzie linijka z plotowaniem wszystkich danych (czyli plt.scatter...). Do tej pory korzystaliście z danych pandowych, więc używaliście 'iloc', teraz wracamy do array'ów zwykłych, więc trzeba iloca się pozbyć :) proponuję zrobić tam if'a."]},{"cell_type":"code","metadata":{"id":"M2qGb3yEJ-OS"},"source":["trainSVM_and_visualize(X, y, 'linear', panda=\"False\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E6RdPOmRz_H_"},"source":["### Co zrobi nam za to SVM jak ustalimy kernel np. gaussowski?\n","Możemy wyobrazić sobie, że te dane są nieźle separowalne, jak zmapujemy je odpowiednią funkcją w wyższy wymiar. Np:"]},{"cell_type":"code","metadata":{"id":"T_9vn2cYKSz5"},"source":["from mpl_toolkits import mplot3d\n","\n","# wchodzimy w 3D za pomocą RBF scentrowanym w środku żółtego bloba\n","r = np.exp(-(X ** 2).sum(1))\n","ax = plt.subplot(projection='3d')\n","ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n","ax.set_xlabel('x')\n","ax.set_ylabel('y')\n","ax.set_zlabel('r')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KxNa4uG4KicO"},"source":["Teraz zrobiliśmy to ręcznie, gdybyśmy chcieli podejść do tego na poważnie, to musielibyśmy zastanowić się nad optymalnym mapowaniem, które najlepiej dzieli klasy, a także nad wydajnością numeryczną takiego mapowania. Wszystko to siedzi w środku SVM, z naszej strony wymaga ona tylko zastanowienia się jakiego kernela chcemy użyć. Polecam wizualizacje pod [tym linkiem](https://www.kaggle.com/joparga3/3-visualising-how-different-kernels-in-svms-work), które dają fajną intuicję czego po różnych kernelach oczekiwać. Ostatecznie jednak warto sprawdzić samą funkcję mapującą."]},{"cell_type":"markdown","metadata":{"id":"d-RcDvB1L968"},"source":["### Użyjcie teraz kernela = 'rbf'"]},{"cell_type":"code","metadata":{"id":"wKjXvpxTMEWh"},"source":["trainSVM_and_visualize(X, y, 'rbf', panda=\"False\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Es83L2Mce1I"},"source":["## Zadanie na dzisiaj\n","Czym są parametry C i gamma w definicji modelu SVM? Proszę znaleźć i opisać za co odpowiadają. Ponadto zadaniem na dzisiaj będzie dobranie optymalnych C i gamma (hiperparametrów). Zanim wyrobi się intuicję czego i jak szukać, można stosować (drogie) podejście systematyczne. Czyli wykonujemy wiele treningów dla ustalonych wartości C i gamma, po czym wyliczamy metryki klasyfikacji dla każdej kombinacji i wizualizujemy najlepsze. To jest zadanie na dzisiaj :)"]},{"cell_type":"markdown","metadata":{"id":"5e6qYJjDO-h0"},"source":["### Będziecie pracować znów na iryskach i znów na dwóch cechach (żeby ułatwić wizualizację), ale tym razem weźmy trzy klasy."]},{"cell_type":"code","metadata":{"id":"fzo9yNOAPHop"},"source":["iris = sns.load_dataset(\"iris\")\n","print(iris.head())\n","\n","y = iris.species\n","y = y.replace('setosa', 0)\n","y = y.replace('versicolor', 1)\n","y = y.replace('virginica', 2)\n","\n","X = iris.drop('species',axis=1)\n","X = X.drop(['sepal_length','sepal_width'], axis=1)\n","X.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PuamGEyhce1V"},"source":["Koniec z czystą wizualizacją, teraz na poważnie dobieramy hiperparametry i oceniamy jakość klasyfikacji. Potrzebujemy podziału na dane treningowe i testowe! (np. za pomocą *train_test_split* z *sklearn.model_selection*)"]},{"cell_type":"code","metadata":{"id":"MRsDiHYDQ8BY"},"source":["X_train, X_test, y_train, y_test = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tdx2As3Rce1u"},"source":["Mając taki podział danych możemy dopasować model SVM do części uczącej. Mamy teraz jednak trzy klasy, a decision_function rysuje się dla jednej klasy vs. pozostałe osobno (mielibyśmy chaotyczny rysunek), więc lepiej zmodyfikować część wizualizacyjną i zamiast tego robić predykcje dla każdego elementu mesha (tak jak na dwóch zajęciach temu, czyli Bayes na iryskach)."]},{"cell_type":"code","metadata":{"id":"eXSXyWHMce1u"},"source":["def trainSVM_and_visualize_multiclass(X_train, Y_train, kernel, C=1.0, gamma='scale')\n","  ...\n","\n","  # Deklaracja obiektu wykres\n","  ...\n","\n","  # Rysujemy wszystkie dane treningowe\n","  ...\n","\n","  # Budujemy siatkę (meshgrid)\n","  h = 0.02 # krok w meshu\n","  x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n","  y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n","  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","  # Robimy predykcję dla każdego elementu meshgridu\n","  Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n","\n","  # Predykcje zmieniamy w kolory\n","  Z = Z.reshape(xx.shape)\n","  plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n","\n","  # Dorysowywujemy wektory wspierające\n","  ...\n","\n","  plt.xlim(xx.min(), xx.max())\n","  plt.ylim(yy.min(), yy.max())\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HxbLb3R4aySG"},"source":["trainSVM_and_visualize_multiclass(X_train, y_train, kernel='rbf')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XnJ0I848ce1w"},"source":["Teraz możemy zastosować miary jakości klasyfikacji."]},{"cell_type":"code","metadata":{"id":"-5zlBnKsaxRW"},"source":["..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FpB62I6Nce10"},"source":["#### Proszę napisać kod, który\n","* skanuje przestrzeń (C, gamma): C w zakresie od 0.1 do 100, gamma w zakresie od 0.1 do 10. Do wygenerowania zakresu ze skalą logarytmiczną można wykorzystać np. takie polecenie: <tt>zakresC = np.logspace(np.log2(0.1),np.log2(100), 8, base=2)</tt>\n","* znajduje najlepsze parametry\n","* rysuje podział przestrzeni dla najlepszych parametrów.\n","\n","Którą miarę jakości będziecie maksymalizować?\n","\n","Stąd możecie czerpać inspirację: [link](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)."]},{"cell_type":"code","metadata":{"id":"nm6cmkZQc96Y"},"source":[""],"execution_count":null,"outputs":[]}]}