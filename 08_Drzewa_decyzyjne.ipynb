{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"08_Drzewa_decyzyjne.ipynb","provenance":[{"file_id":"1lATP2vPWr0aFkJy21UxAD_6R68tBe_it","timestamp":1607687665896},{"file_id":"1HivAOxW2Q3XXsug-ZEicFkuLU5i0JjcK","timestamp":1607683292832}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["---\n","# Drzewa decyzyjne #\n","---\n","\n","## 1. Pobranie i analiza danych uczących ##\n","Będziemy dziś pracować na [zbiorze danych](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29) uzyskanych z biopsji (aspiracyjnej cienkoigłowej) piersi. Zadaniem naszym jest wytrenowanie klasyfikatora, który po cechach wyników biopsji będzie klasyfikował guz jako łagodny lub złośliwy."],"metadata":{"id":"QRL0lkko_nqm"}},{"cell_type":"code","execution_count":1,"source":["from sklearn.datasets import load_breast_cancer\r\n","\r\n","# Gdy znacie zestaw danych i chcecie od razu przejść do treningu, opcja 'return_X_y = True' jest dla Was!\r\n","#X, y = load_breast_cancer(return_X_y=True)\r\n","\r\n","# Ale to nasz pierwszy raz z tym zestawem danych, więc sprawdźmy co jest w środku. Wykorzystajcie analizę danych zrobioną dla irysów w notebooku z naiwnym klasyfikatorem Bayesa.\r\n","cancer = load_breast_cancer()\r\n","\r\n","# Sprawdźcie opis zestawu danych, a także nazwy klas i nazwy cech\r\n","# Sprawdźcie ile jest przykładów z obu klas\r\n","# Zróbcie violinploty dla wszystkich cech LUB przyjrzyjcie się zakresom liczbowym cech. \r\n","# Co o nich myślicie? Czy sprawią jakiś problem? Czy powinniśmy coś z nimi zrobić? PODPOWIEDŹ: TAK ;D\r\n","# Jeśli nie możecie wymyślić gdzie leży pułapka, to dajcie mi znać, nie idźcie dalej, wymyślimy to razem :)"],"outputs":[],"metadata":{"id":"lCoIbcPgJ670","executionInfo":{"status":"ok","timestamp":1607687732252,"user_tz":-60,"elapsed":1530,"user":{"displayName":"Anna Dawid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64","userId":"02862484648310443813"}}}},{"cell_type":"code","execution_count":null,"source":["# Zróbcie wykres korelacji między cechami (tym razem prawdziwej korelacji ;) nie jak w irysach!\r\n","# X zawiera Wasze dane uczące\r\n","X, y = load_breast_cancer(return_X_y=True)\r\n","\r\n","plt.figure()\r\n","f, ax = plt.subplots(figsize=(14,14))\r\n","corr_plot = sns.heatmap(np.corrcoef(X, rowvar=False), annot=False, linewidths=.5, fmt='.1f', ax=ax)\r\n","\r\n","# Czego się dowiedzieliście z tego wykresu?"],"outputs":[],"metadata":{"id":"4V6PhzsbKIP4"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.model_selection import train_test_split\r\n","\r\n","# Podziel dane na treningowe i testowe\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"],"outputs":[],"metadata":{"id":"HoGUGjn6dD6-"}},{"cell_type":"code","execution_count":null,"source":["# Tutaj unikamy pułapki z komórki nr 1 :)"],"outputs":[],"metadata":{"id":"JqTBO4Mof2tl"}},{"cell_type":"markdown","source":["## 2. Importujemy klasyfikator w postaci drzewa decyzyjnego ##\n","W bibliotece scikit-learn drzewa decyzyjne implementowane są przez klasę [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). Szczegóły implementacji opisane są [tutaj](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use).\n","\n","Najprostszy przykład:"],"metadata":{"id":"KbCV0TYNJw49"}},{"cell_type":"code","execution_count":null,"source":["from sklearn import tree\r\n","X = [[0, 0], [1, 1]]\r\n","Y = [0, 1]\r\n","clf = tree.DecisionTreeClassifier()\r\n","clf = clf.fit(X, Y)"],"outputs":[],"metadata":{"id":"MUxTIfO__3kf"}},{"cell_type":"markdown","source":["Po dopasowaniu można przewidywać przynależność nowych przykładów:"],"metadata":{"id":"2HXxAKlsA5ig"}},{"cell_type":"code","execution_count":null,"source":["clf.predict([[2., 2.]])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1])"]},"metadata":{"tags":[]},"execution_count":28}],"metadata":{"id":"SrNHQGZTA6Kf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607685371801,"user_tz":-60,"elapsed":886,"user":{"displayName":"Anna Dawid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64","userId":"02862484648310443813"}},"outputId":"82e1d2bb-7ad6-4756-a661-7eaf08b3b438"}},{"cell_type":"markdown","source":["Albo estymować prawdopodobieństwo przynależności do klas:"],"metadata":{"id":"_L4pVDROA7-V"}},{"cell_type":"code","execution_count":null,"source":["clf.predict_proba([[2., 2.]])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1.]])"]},"metadata":{"tags":[]},"execution_count":29}],"metadata":{"id":"9sxm8154A-Q5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607685374872,"user_tz":-60,"elapsed":913,"user":{"displayName":"Anna Dawid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUQAwZ7wyayL4BbiM0n_EANCgBjSdZ9H14lgcCFEE=s64","userId":"02862484648310443813"}},"outputId":"2fc477c9-9176-4395-be75-b5be8de3a016"}},{"cell_type":"markdown","source":["## 3. Klasyfikacja guzów i wizualizacja drzewa ##"],"metadata":{"id":"AAHv_u_OBHm4"}},{"cell_type":"code","execution_count":null,"source":["# Deklarujemy klasyfikator\r\n","\r\n","# Fitujemy do danych treningowych\r\n"],"outputs":[],"metadata":{"id":"jpVmJ-GOBLvh"}},{"cell_type":"markdown","source":["Po wytrenowaniu można zilustrować wynik za pomocą narzędzia Graphiz (wymaga to zainstalowania w systemie tego narzędzia), oraz doinstalowania do pythona biblioteki pydot:"],"metadata":{"id":"ID0_izjhBWnu"}},{"cell_type":"code","execution_count":null,"source":["# Nazwałam tu klasyfikator 'clf'. Jeśli zadeklarowaliście go pod inną nazwą, to:\r\n","# clf = Wasza_Nazwa\r\n","\r\n","from six import StringIO\r\n","import pydot \r\n","dot_data = StringIO() \r\n","tree.export_graphviz(clf, out_file=dot_data) \r\n","graph = pydot.graph_from_dot_data(dot_data.getvalue()) \r\n","graph[0].write_pdf(\"iris.pdf\")"],"outputs":[],"metadata":{"id":"ZI07lpmKBOSc"}},{"cell_type":"markdown","source":["Można też podejrzeć wyniki w pythonie:"],"metadata":{"id":"doq0jo-pB38S"}},{"cell_type":"code","execution_count":null,"source":["from IPython.display import Image  \r\n","dot_data = StringIO()  \r\n","tree.export_graphviz(clf, out_file=dot_data, feature_names=cancer.feature_names,  \r\n","                         class_names=cancer.target_names,  \r\n","                         filled=True, rounded=True,  \r\n","                         special_characters=True) \r\n","graph = pydot.graph_from_dot_data(dot_data.getvalue())  \r\n","Image(graph[0].create_png())"],"outputs":[],"metadata":{"id":"VMuw7pNhB1ib"}},{"cell_type":"markdown","source":["Czym jest \"Gini\", \"samples\", \"value\" oraz \"class\"? Co oznaczają kolory?"],"metadata":{"id":"VI84Ra6hCKDk"}},{"cell_type":"code","execution_count":null,"source":["# Alternatywą jest użycie wbudowanej metody 'plot_tree', ale wydaje mi się mniej przyjazna:\r\n","tree.plot_tree(clf, feature_names=cancer.feature_names)"],"outputs":[],"metadata":{"id":"tpB2b0cYCLUw"}},{"cell_type":"markdown","source":["Oczywiście oceniamy jakość tej klasyfikacji:"],"metadata":{"id":"AZwZQFtajlhI"}},{"cell_type":"code","execution_count":null,"source":["# Raport jakości klasyfikacji + Wasze ulubione miary jakości."],"outputs":[],"metadata":{"id":"4qy16AOcjpcz"}},{"cell_type":"markdown","source":["Co o niej myślicie? A sprawdźcie na wszelki wypadek klasyfikację danych treningowych?"],"metadata":{"id":"9IsI-AiwkaVv"}},{"cell_type":"code","execution_count":null,"source":["# Raport klasyfikacji w procesie treningowym"],"outputs":[],"metadata":{"id":"1dZlSeD0kewy"}},{"cell_type":"markdown","source":["Co to oznacza? :)"],"metadata":{"id":"t9TmlcP7kmJs"}},{"cell_type":"markdown","source":["## 4. Regularyzacja drzewa przez przycinanie##\n","\n","A teraz crème de la crème uczenia maszynowego, czyli regularyzacja. Pamiętacie na czym ogólnie polega?\n","\n","W przypadku drzew decyzyjnych polega przede wszystkim na:\n","- przycinaniu gałęzi i liści (czyli zmniejszaniu liczby węzłów) przez kontrolę parametru min_samples_leaf: mianowicie daje dolny limit na jak drobne gałązki może drzewo tworzyć. Jeśli kolejna gałązka miałaby rozpatrywać tylko liczbę przykładów < min_sample_leaf, to nie tworzy się. Domyślnie, min_sample_leaf = ?\n","- ograniczaniu głębokości drzewa, czyli jak wiele rozgałęzień może się stworzyć. Domyślnie max_depth = None. Co to znaczy? Patrz [tu](https://scikit-learn.org/0.22/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier).\n","\n","Macie dwa wyjścia. Po pierwsze, poszukać optymalnych hiperparametrów ręcznie, jak ostatnio robiliście. Po drugie, skorzystać (z rozwagą!) z gotowej metody cost_complexity_pruning_path, w której bawicie się jednym parametrem 'cost complexity parameter', czyli ccp_alpha. Tutaj macie [tutorial](https://scikit-learn.org/0.22/auto_examples/tree/plot_cost_complexity_pruning.html). Jeśli będziecie korzystać z tej funkcji, to proszę o bogate komentarze, co się po drodze dzieje :)\n","\n","Ostatecznym wynikiem ma być uzyskanie optymalnego drzewa oraz jego wizualizacja wraz z raportem klasyfikacji i macierzą pomyłek.\n","\n","Powodzenia! :)"],"metadata":{"id":"1H63F62rCRRz"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"uzzAQE-yrElW"}}]}