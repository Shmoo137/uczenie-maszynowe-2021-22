{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Template] 02_Regresja_liniowa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fVD-WRKhgE2"
      },
      "source": [
        "Importujemy ponownie powtórzone `numpy`, dodajemy moduł do tworzenia wykresów `pyplot`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB6NUjsLhTLd"
      },
      "source": [
        "import numpy as np\n",
        "import pylab as py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE_3hFYtIDv-"
      },
      "source": [
        "# Zapoznanie się z regresją liniową\n",
        "* W ramach tego ćwiczenia będziemy chcieli opisać zbiór danych modelem liniowym.\n",
        "* Zbiór danych stworzymy sami w sposób sztuczny, ale w typowych problemach zebranie i obróbka danych stanowi znaczącą część pracy.\n",
        "* Nasz liniowy model ma postać: $y = \\theta_0 + \\theta_1 x$\n",
        "* Dane wytworzymy dla konkretnych $\\theta_0$ i $\\theta_1$, a następnie zaimplementujemy regresję liniową, aby znaleźć jak najlepsze przybliżenie dla tych parametrów.\n",
        "* `(X,Y)` to ciąg uczący. *Co to ciąg uczący?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PHcEKITkFAc"
      },
      "source": [
        "## Produkcja danych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnbVmOFqIDv_"
      },
      "source": [
        "Zacznijmy od produkcji $x$. W późniejszej części ćwiczenia zobaczymy, że wygodniej je mieć w postaci wektora kolumnowego, można użyć `reshape` teraz lub później:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttS1qAmEIDwA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0e4bcbb7-3f88-442b-ee0f-0710ddec245a"
      },
      "source": [
        "ile = 10 \n",
        "x = np.linspace(0, 10, ile) #.reshape(ile,1)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.          1.11111111  2.22222222  3.33333333  4.44444444  5.55555556\n",
            "  6.66666667  7.77777778  8.88888889 10.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwGOuzcQIDwH"
      },
      "source": [
        "Ustalamy parametry dla symulacji $\\theta_0 = 1$ i $\\theta_1 = 3$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8makH4WzIDwI"
      },
      "source": [
        "theta0 = 1\n",
        "theta1 = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3l9-73PIDwD"
      },
      "source": [
        "Teraz produkujemy odpowiadające $y$ korzystając z wybranych parametrów."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYjzz8F34-xz"
      },
      "source": [
        "y ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yewJ96CHIDwL"
      },
      "source": [
        "Obejrzyjmy te dane:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL_rSguzmlKi"
      },
      "source": [
        "py.plot(x, y, 'bo')\n",
        "py.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StUPjPEbmyO5"
      },
      "source": [
        "Skomplikujmy trochę zadanie dla naszej regresji. Wprowadźmy coś na kształt szumu, dodając do każdego $y$ małą liczbę losową z N(0,1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWMWQc7SIDwL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_r-QQtsIDwR"
      },
      "source": [
        "## Algorytm równań normalnych\n",
        "Proszę napisać funkcję, która:\n",
        "* na wejściu przyjmuje ciąg uczący, implementuje wzór na parametry optymalne na podstawie [równań normalnych](http://brain.fuw.edu.pl/edu/index.php/Uczenie_maszynowe_i_sztuczne_sieci_neuronowe/Wykład_1#Minimalizacja_funkcji_kosztu ). \n",
        "* Funkcja powinna zwracać estymowane parametry theta.\n",
        "* Proszę dorysować prostą reprezentującą hipotezę do wykresu punktów ciągu uczącego.\n",
        "* dla przypomnienia: odwrotność macierzy można obliczyć w numpy funkcją: <tt>numpy.linalg.inv</tt>\n",
        "* proszę zwrócić uwagę, że konieczne jest użycie wektorów kolumnowych!\n",
        "\n",
        "Podpowiedź: aby skorzystać ze wzorów z wykładu, macierz wejść $X$ musi zawierać nie tylko kolumnę $x$, ale także kolumnę jedynek, aby przemnożona przez wektor [$\\theta_0$ $\\theta_1$] dawała odpowiedni wektor (kolumnowy) wyjść $y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pK1Ue7oIDwS"
      },
      "source": [
        "def licz_rownania_normalne(X, y):\n",
        "    ...\n",
        "    return theta\n",
        "\n",
        "X = \n",
        "Y = \n",
        "theta = \n",
        "\n",
        "theta_est = licz_rownania_normalne(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HA0EFYBIDwV"
      },
      "source": [
        "## Algorytm gradientowy stochastyczny \n",
        "Proszę napisać funkcję, która znajduje optymalne parametry theta wg algorytmu gradientowego stochastycznego (metoda najmniejszych kwadratów). Funkcja jako argumenty przyjmuje ciąg uczący, (dowolne) wartości początkowe theta i parametr szybkości zbiegania alpha.\n",
        "\n",
        "Na wyjściu funkcja powinna zwracać wyestymowane wartości parametrów.\n",
        "\n",
        "W ramach ilustracji po każdej iteracji proszę dorysować prostą parametryzowaną przez aktualne wartości parametrów. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Sn8K1uIIDwW"
      },
      "source": [
        "def licz_iteracyjnie_stoch(X, Y, theta0 = np.array([?,?]).reshape(2,1), alpha = ?, epochs = ?):\n",
        "    for i in range(epochs):\n",
        "        ...\n",
        "    return theta0\n",
        "\n",
        "theta_est = licz_iteracyjnie_stoch(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPd4lLdUIDwZ"
      },
      "source": [
        "## Algortym gradientowy zbiorczy\n",
        "Proszę napisać funkcję, która znajduje optymalne parametry theta wg. algorytmy gradientowego zbiorczego. Funkcja jako argumenty przyjmuje ciąg uczący, wartości początkowe theta i parametr szybkości zbiegania alpha.\n",
        "Na wyjściu funkcja powinna zwracać wyestymowane wartości parametrów.\n",
        "\n",
        "W ramach ilustracji po każdej iteracji proszę dorysować prostą parametryzowaną przez aktualne wartości parametrów. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi3KFdUbIDwb"
      },
      "source": [
        "def licz_iteracyjnie_batch(X, Y, theta0 = np.array([?,?]).reshape(2,1), alpha = ?, epochs = ?):\n",
        "    ...\n",
        "    return theta0\n",
        "\n",
        "theta_est = licz_iteracyjnie_batch(X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKgTHdG9IDwe"
      },
      "source": [
        "## Porównanie algorytmów\n",
        "* Który z algorytmów gradientowych zbiega szybciej do minimum?\n",
        "* Jakie są zalety i wady obu algorytmów?"
      ]
    }
  ]
}